{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn\n",
    "import torch.autograd\n",
    "\n",
    "import numpy\n",
    "\n",
    "import Utils\n",
    "\n",
    "class CircularConv2d(torch.nn.Conv2d):\n",
    "\n",
    "    def __init__(self, \n",
    "            in_channels, \n",
    "            out_channels, \n",
    "            kernel_size, \n",
    "            stride=1, \n",
    "            dilation=1, \n",
    "            groups=1, \n",
    "            bias=True, \n",
    "            padding_mode='zeros', \n",
    "            device=None, \n",
    "            dtype=None\n",
    "        ) -> None:\n",
    "        \n",
    "        self.kernel_size = kernel_size\n",
    "        \n",
    "        super(CircularConv2d, self).__init__(\n",
    "            in_channels=in_channels, \n",
    "            out_channels=out_channels, \n",
    "            kernel_size=kernel_size, \n",
    "            stride=stride, \n",
    "            padding='valid', \n",
    "            dilation=dilation, \n",
    "            groups=groups, \n",
    "            bias=bias, \n",
    "            padding_mode=padding_mode, \n",
    "            device=device, \n",
    "            dtype=dtype\n",
    "        )\n",
    "        \n",
    "    def T(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x_flipped = torch.flip(x, dims=[0, 1])\n",
    "        out = self.forward(x_flipped)\n",
    "        return torch.flip(out, dims=[0, 1])\n",
    "        \n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        #pad_width = [ (0, 0) ] + [  (i // 2, i // 2) for i in self.kernel_size ]\n",
    "        n = self.kernel_size[0] // 2\n",
    "        m = self.kernel_size[1] // 2\n",
    "        # pad_width = [ (0, 0) ] + [ (i // 2, i // 2) for i in self.kernel_size ]\n",
    "        pad_width = [ (0, 0), (n, n), (m, m) ]\n",
    "        x_padded = numpy.pad(x.cpu().detach().numpy(), pad_width=pad_width , mode='wrap')\n",
    "        x_padded = torch.tensor(x_padded, dtype= x.dtype, device=x.device)\n",
    "        return super().forward(x_padded)\n",
    "\n",
    "\n",
    "# class Kernel(torch.nn.Module):\n",
    "\n",
    "#     def __init__(self, \n",
    "#         in_channels: int,\n",
    "#         out_channels: int,\n",
    "#         kernel_size: tuple\n",
    "#     ) -> None:\n",
    "\n",
    "#         super(Kernel, self).__init__()\n",
    "\n",
    "#         self.conv = torch.nn.Conv2d(\n",
    "#             in_channels=in_channels,\n",
    "#             out_channels=out_channels,\n",
    "#             kernel_size=kernel_size,\n",
    "#             stride=1,\n",
    "#             padding='same',\n",
    "#             dilation=1,\n",
    "#             bias=False\n",
    "#         )\n",
    "\n",
    "#         # torch.nn.init.zeros_(self.conv.weight.data)\n",
    "#         torch.nn.init.constant_(self.conv.weight.data, 1.0/(kernel_size[0]*kernel_size[0]))\n",
    "#         # torch.nn.init.xavier_uniform_(self.conv.weight, gain=1.0)\n",
    "\n",
    "#         self.batchnorm = torch.nn.BatchNorm2d(\n",
    "#             num_features=out_channels,\n",
    "#             eps=1e-05,\n",
    "#             momentum=1e-1,\n",
    "#             affine=True\n",
    "#         )\n",
    "\n",
    "#         self.activation = torch.nn.Sigmoid()\n",
    "\n",
    "#         # self.dropout = torch.nn.Dropout(p=0.2)\n",
    "\n",
    "#     def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "\n",
    "#         out = self.conv(x)\n",
    "#         # out = self.batchnorm(out.unsqueeze(0)).squeeze(0)\n",
    "#         # out = self.activation(out)\n",
    "#         # # out = self.dropout(out4)\n",
    "\n",
    "        \n",
    "#         return out\n",
    "\n",
    "      \n",
    "\n",
    "class Iteration(torch.nn.Module):\n",
    "\n",
    "    # Static attribute\n",
    "    # <=> if attributs change, all instance change\n",
    "    # <=> attribute shared between all \"Iteration\" object \n",
    "    # https://docs.python.org/3/tutorial/classes.html#class-and-instance-variables\n",
    "    # This attribute are not learnable\n",
    "    #d_x: torch.Tensor # Shared\n",
    "    #d_y: torch.Tensor # Shared\n",
    "    #b_x: torch.Tensor # Shared\n",
    "    #b_y: torch.Tensor # Shared\n",
    "    # f: torch.Tensor # shared\n",
    "\n",
    "    def __init__(self, \n",
    "        nb_intermediate_channels: int, \n",
    "        kernel_size: tuple,\n",
    "        alpha: float,\n",
    "        beta0: float,\n",
    "        beta1: float,\n",
    "        sigma: float,\n",
    "        alpha_learnable: bool,\n",
    "        beta0_learnable: bool,\n",
    "        beta1_learnable: bool,\n",
    "        sigma_learnable: bool,\n",
    "        taylor_nb_iterations: int,\n",
    "        taylor_kernel_size: tuple\n",
    "    ) -> None:\n",
    "\n",
    "        super(Iteration, self).__init__()\n",
    "        # f_approx = argmin { \n",
    "        #   (alpha / 2) || g - Hf ||^{2}_{2}\n",
    "        #   + (beta0 / 2) || nabla f ||^{2}_{2}\n",
    "        #   + beta1 || nabla f ||_{1}\n",
    "\n",
    "        self.nb_intermediate_channels = nb_intermediate_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.n = taylor_nb_iterations\n",
    "        \n",
    "        # Hyper-parameters\n",
    "        # self.alpha = torch.nn.Parameter(data=torch.abs(torch.randn(1, dtype=torch.float)), requires_grad=True)\n",
    "        # self.beta1 = torch.nn.Parameter(data=torch.abs(torch.randn(1, dtype=torch.float)), requires_grad=True)\n",
    "        self.alpha = torch.nn.Parameter(\n",
    "            data=torch.tensor([alpha], dtype=torch.float),\n",
    "            requires_grad=alpha_learnable\n",
    "        )\n",
    "\n",
    "        self.beta0 = torch.nn.Parameter(\n",
    "            data=torch.tensor([beta0], dtype=torch.float),\n",
    "            requires_grad=beta0_learnable\n",
    "        )\n",
    "\n",
    "        self.beta1 = torch.nn.Parameter(\n",
    "            data=torch.tensor([beta1], dtype=torch.float),\n",
    "            requires_grad=beta1_learnable\n",
    "        )\n",
    "\n",
    "        self.sigma = torch.nn.Parameter(\n",
    "            data=torch.tensor([sigma], \n",
    "            dtype=torch.float), \n",
    "            requires_grad=sigma_learnable\n",
    "        )\n",
    "\n",
    "        # # H\n",
    "        self.h = CircularConv2d(\n",
    "            in_channels=1,\n",
    "            out_channels=1,\n",
    "            kernel_size=kernel_size,\n",
    "            bias=False\n",
    "        )\n",
    "\n",
    "        # torch.nn.init.xavier_uniform_(self.h.weight.data)\n",
    "        # self.batchnorm = torch.nn.BatchNorm2d(\n",
    "        #     num_features=out_channels,\n",
    "        #     eps=1e-05,\n",
    "        #     momentum=1e-1,\n",
    "        #     affine=True\n",
    "        # )\n",
    "\n",
    "        # self.activation = torch.nn.Tanh()\n",
    "        \n",
    "        #self.activation = torch.nn.LeakyReLU(0.1)\n",
    "        \n",
    "        \n",
    "        # self.relu = torch.nn.ReLU()\n",
    "        # threshold = 1e-10\n",
    "        # value = threshold\n",
    "        # self.threshold = torch.nn.Threshold(threshold, value, inplace=False)\n",
    "        # self.parameter_activation = torch.nn.Threshold(threshold, value, inplace=False)\n",
    "        # self.parameter_activation = torch.nn.Sigmoid()\n",
    "        # self.parameter_activation = lambda x : x\n",
    "        # self.taylor_activation = torch.nn.Tanh()\n",
    "        # self.taylor_activation = torch.nn.LeakyReLU(negative_slope=1e-1)\n",
    "        # self.taylor_activation = torch.nn.Sigmoid()\n",
    "        # self.taylor_activation = lambda u : u\n",
    "        # torch.nn.Hardtanh(min_val, max_val, inplace=False)\n",
    "\n",
    "        # self.ht_decim_activation = torch.nn.Sigmoid()\n",
    "        # self.ht_decim_activation = lambda x : x\n",
    "\n",
    "\n",
    "         \n",
    "    def forward(self, STg, decim_row, decim_col, d_x, d_y, b_x, b_y) -> torch.Tensor:\n",
    "        \n",
    "        # STg = S^{T} g \n",
    "        #print('STg :', STg)\n",
    "        #print(self.sigma.device)\n",
    "        \n",
    "        # COMPUTE f approximation\n",
    "        gradT_x = Utils.dxT(d_x - b_x)\n",
    "        # gradT_x_normalized = Utils.matrix_normalize(gradT_x)\n",
    "        # print('gradT_x :', gradT_x, torch.min(gradT_x), torch.max(gradT_x))\n",
    "        ## = (nabla_x)^{T} (d_x - b_x)\n",
    "        gradT_y = Utils.dyT(d_y - b_y)\n",
    "        # gradT_y_normalized = Utils.matrix_normalize(gradT_y)\n",
    "        # print('gradT_y :', gradT_y, torch.min(gradT_y), torch.max(gradT_y))\n",
    "        ## = (nabla_y)^{T} (d_y - b_y)\n",
    "        sigma_expr = self.sigma * ( gradT_x + gradT_y )\n",
    "        # sigma_expr = self.sigma * ( gradT_x_normalized + gradT_y_normalized )\n",
    "        # sigma_expr = self.sigma_x * gradT_x_normalized + self.sigma_y * gradT_y_normalized\n",
    "        # print('sigma_expr :', sigma_expr, torch.min(sigma_expr), torch.max(sigma_expr))\n",
    "        ## = sigma * [ (nabla_x)^{T} (d_x - b_x) + (nabla_y)^{T} (d_y - b_y) ]\n",
    "        alpha_expr = self.alpha * self.h.T(STg.unsqueeze(0))\n",
    "        # print('alpha_expr :', alpha_expr, torch.min(alpha_expr), torch.max(alpha_expr))\n",
    "        # alpha_expr = alpha_expr.squeeze(0)\n",
    "        ## = alpha * (H^{T} S^{T} g)\n",
    "        # Iteration.f = self.conv1((sigma_expr + alpha_expr).unsqueeze(0))\n",
    "        # f = self.inv(sigma_expr + alpha_expr)\n",
    "        # f = f.squeeze(0)\n",
    "        # if gradT_x.isnan().any():\n",
    "        #     raise AssertionError('gradT_x : Nan ')\n",
    "        # if gradT_y.isnan().any():\n",
    "        #     raise AssertionError('gradT_y : Nan ')\n",
    "                \n",
    "        # if (sigma_expr + alpha_expr).isnan().any():\n",
    "        #     raise AssertionError('Nan before inversion')\n",
    "        f = self.taylor_young_ld(\n",
    "            x = (sigma_expr + alpha_expr).squeeze(0), \n",
    "            decim_row = decim_row,\n",
    "            decim_col = decim_col,\n",
    "            n = self.n\n",
    "        )\n",
    "        # print('f :', f, torch.min(f), torch.max(f))\n",
    "        # if f.isnan().any():\n",
    "        #     raise AssertionError('Nan after inversion')\n",
    "\n",
    "        ## = [ alpha H^{T} S^{T} S H + (beta0 + sigma) laplacian ]^{-1}\n",
    "        ##  * (\n",
    "        ##      sigma * [ (nabla_x)^{T} (d_x - b_x) + (nabla_y)^{T} (d_y - b_y) ]\n",
    "        ##      + alpha * (H^{T} S^{T} g)\n",
    "        ##  )\n",
    "\n",
    "        # Update (d_x, d_y) : Multidimensional Soft Thresholding\n",
    "        dx_f = Utils.dx(f)\n",
    "        # print('dx_f :', dx_f, torch.min(dx_f), torch.max(dx_f))\n",
    "        dy_f = Utils.dy(f)\n",
    "        # print('dy_f :', dy_f, torch.min(dy_f), torch.max(dy_f))\n",
    "\n",
    "        # if dx_f.isnan().any():\n",
    "        #     raise AssertionError('dx_f : Nan ')\n",
    "        # if dy_f.isnan().any():\n",
    "        #     raise AssertionError('dy_f : Nan ')\n",
    "        \n",
    "        # d_x = Utils.soft((dx_f + b_x).unsqueeze(0), self.beta1 / self.sigma_x)\n",
    "        # d_y = Utils.soft((dy_f + b_y).unsqueeze(0), self.beta1 / self.sigma_y)\n",
    "\n",
    "        d_x, d_y = Utils.multidimensional_soft(\n",
    "            torch.concat(\n",
    "                [ \n",
    "                    (dx_f + b_x).unsqueeze(0), \n",
    "                    (dy_f + b_y).unsqueeze(0)\n",
    "                ],\n",
    "                0\n",
    "            ),\n",
    "            self.beta1 / self.sigma\n",
    "            # self.parameter_activation(self.beta1 / self.sigma)\n",
    "        )\n",
    "        # print('d_x :', d_x)\n",
    "        # print('d_y :', d_y)\n",
    "\n",
    "\n",
    "        # if d_x.isnan().any():\n",
    "        #     raise AssertionError('d_x : Nan ')\n",
    "        # if d_y.isnan().any():\n",
    "        #     raise AssertionError('d_y : Nan ')\n",
    "      \n",
    "        # Update (b_x, b_y)\n",
    "        b_x += (dx_f - d_x)\n",
    "        b_y += (dy_f - d_y)\n",
    "\n",
    "        # print('b_x :', b_x)\n",
    "        # print('b_y :', b_y)\n",
    "\n",
    "        # print()\n",
    "        # print()\n",
    "        # print()\n",
    "        # print()\n",
    "        # print()\n",
    "        \n",
    "        return [ f, d_x, d_y, b_x, b_y ]\n",
    "\n",
    "\n",
    "    def taylor_young_ld(self, x: torch.Tensor, decim_row: int, decim_col: int, n: int) -> torch.Tensor:\n",
    "\n",
    "        # k = 0\n",
    "        ld = x\n",
    "        for k in range(1, n+1):\n",
    "            # ld = x - self.taylor_activation(self.compute(ld, decim_row, decim_col))\n",
    "            ld = x - self.compute(ld, decim_row, decim_col)\n",
    "            # print('x :', x)\n",
    "\n",
    "        # return self.taylor_activation(ld)\n",
    "        return ld\n",
    "\n",
    "    def compute(self, u: torch.Tensor, decim_row: int, decim_col: int) -> torch.Tensor:\n",
    "        \"\"\"Computes :\n",
    "        [I - (alpha H^{T} S^{T} S H + (beta0 + sigma) laplacian)] u\n",
    "        = u - [(alpha H^{T} S^{T} S H] u - [(beta0 + sigma) laplacian)] u\n",
    "        \"\"\"\n",
    "        \n",
    "        # [ alpha H^{T} S^{T} S H ] u\n",
    "        out1 = self.h(u.unsqueeze(0))\n",
    "        out2 = Utils.decimation(out1.squeeze(0), decim_row, decim_col)\n",
    "        out3 = Utils.decimation_adjoint(out2, decim_row, decim_col)\n",
    "        out4 = self.h.T(out3.unsqueeze(0))\n",
    "        out5 = self.alpha * out4\n",
    "        term1 = out5.squeeze(0)\n",
    "\n",
    "        # [(beta0 + sigma) laplacian ] u\n",
    "        laplacian = Utils.laplacian2D_v2(u)\n",
    "        term2 = (self.beta0 + self.sigma) * laplacian\n",
    "      \n",
    "        # [ I - (alpha H^{T} S^{T} S H + (beta0 + sigma) laplacian) ] u\n",
    "        #= u - [ (alpha H^{T} S^{T} S H ] u - [ (beta0 + sigma) laplacian) ] u\n",
    "        res = u - term1 - term2\n",
    "\n",
    "        return res\n",
    "\n",
    "\n",
    "class Unfolding(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, \n",
    "        nb_intermediate_channels: int,\n",
    "        kernel_size: tuple,\n",
    "        nb_iterations: int,\n",
    "        alpha: float,\n",
    "        beta0: float,\n",
    "        beta1: float,\n",
    "        sigma: float,\n",
    "        alpha_learnable: bool,\n",
    "        beta0_learnable: bool,\n",
    "        beta1_learnable: bool,\n",
    "        sigma_learnable: bool,\n",
    "        taylor_nb_iterations: int,\n",
    "        taylor_kernel_size: tuple\n",
    "    ) -> None:\n",
    "        \n",
    "        super(Unfolding, self).__init__()\n",
    "        \n",
    "        params = [\n",
    "            nb_intermediate_channels,\n",
    "            kernel_size,\n",
    "            alpha,\n",
    "            beta0,\n",
    "            beta1,\n",
    "            sigma,\n",
    "            alpha_learnable,\n",
    "            beta0_learnable,\n",
    "            beta1_learnable,\n",
    "            sigma_learnable,\n",
    "            taylor_nb_iterations,\n",
    "            taylor_kernel_size\n",
    "        ]\n",
    "\n",
    "\n",
    "        iters = [ Iteration(*params) for _ in range(0, nb_iterations) ]\n",
    "        \n",
    "        # self.iterations = torch.nn.Sequential(*iters)\n",
    "        \n",
    "        self.iterations = torch.nn.ModuleList(iters)\n",
    "\n",
    "\n",
    "    def forward(self, \n",
    "        low_resolution: torch.Tensor,\n",
    "        decim_row: int,\n",
    "        decim_col: int\n",
    "    ) -> torch.Tensor:\n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "            Params:\n",
    "                - low_resolution : image low-resolution\n",
    "                - decim_row : decimation on line\n",
    "                - decim_col : decimation on col\n",
    "\n",
    "            Return:\n",
    "                Image high-resolution of size.\n",
    "                If size of low_resolution is (N, M), Image high-resolution\n",
    "                will be (N*decim_row, M*decim_col)\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "        # Initialize static attribute / shared attribute\n",
    "\n",
    "        g = low_resolution\n",
    "        STg = Utils.decimation_adjoint(g, decim_row, decim_col)\n",
    "        \n",
    "        d_x = torch.zeros_like(STg)\n",
    "        d_y = torch.zeros_like(STg)\n",
    "        b_x = torch.zeros_like(STg)\n",
    "        b_y = torch.zeros_like(STg)\n",
    "\n",
    "        # _ = self.iterations(inputs)\n",
    "        for iter_layer in self.iterations:\n",
    "            # STg, d_x, d_y, b_x, b_y = iter_layer(STg, d_x, d_y, b_x, b_y)\n",
    "            f, d_x, d_y, b_x, b_y = iter_layer(STg, decim_row, decim_col, d_x, d_y, b_x, b_y)\n",
    "\n",
    "        # f_approx = Iteration.f\n",
    "        f_approx = f\n",
    "        # Normalize f\n",
    "        mini = torch.min(f_approx)\n",
    "        maxi = torch.max(f_approx)\n",
    "        normalized = (f_approx - mini) / (maxi - mini)\n",
    "\n",
    "        return normalized\n",
    "\n",
    "    @classmethod\n",
    "    def from_config(cls, config: dict) -> 'Unfolding':\n",
    "\n",
    "        model_config = config['model']\n",
    "        params = model_config['params']\n",
    "\n",
    "        nb_intermediate_channels = params['nb_intermediate_channels']\n",
    "        kernel_size = params['kernel_size']\n",
    "        nb_iterations = params['nb_iteration']\n",
    "\n",
    "        alpha = params['alpha']['initialize']\n",
    "        beta0 = params['beta0']['initialize']\n",
    "        beta1 = params['beta1']['initialize']\n",
    "        sigma = params['sigma']['initialize']\n",
    "\n",
    "        alpha_learnable = params['alpha']['is_learnable']\n",
    "        beta0_learnable = params['beta0']['is_learnable']\n",
    "        beta1_learnable = params['beta1']['is_learnable']\n",
    "        sigma_learnable = params['sigma']['is_learnable']\n",
    "\n",
    "        taylor_nb_iterations = params['taylor']['nb_iteration']\n",
    "        taylor_kernel_size = params['taylor']['kernel_size']\n",
    "\n",
    "        params = [\n",
    "            nb_intermediate_channels,\n",
    "            kernel_size,\n",
    "            nb_iterations,\n",
    "            alpha,\n",
    "            beta0,\n",
    "            beta1,\n",
    "            sigma,\n",
    "            alpha_learnable,\n",
    "            beta0_learnable,\n",
    "            beta1_learnable,\n",
    "            sigma_learnable,\n",
    "            taylor_nb_iterations,\n",
    "            taylor_kernel_size\n",
    "        ]\n",
    "\n",
    "        model = Unfolding(*params)\n",
    "        device = model_config['device']\n",
    "\n",
    "        return model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import sys\n",
    "sys.path.append('../..')\n",
    "\n",
    "import lasp.torch.autograd_ops\n",
    "import lasp.torch.differential\n",
    "import lasp.torch.utils\n",
    "import typing\n",
    "\n",
    "def dx(tensor: torch.Tensor) -> torch.Tensor:\n",
    "\n",
    "    if tensor.requires_grad:\n",
    "        grad_x = lasp.torch.autograd_ops.GradX2DCirc.apply(\n",
    "            tensor\n",
    "        )\n",
    "    else:\n",
    "        grad_x = lasp.torch.differential.dx2D_circ(\n",
    "            tensor,\n",
    "            detach = True\n",
    "        )\n",
    "\n",
    "    return grad_x\n",
    "\n",
    "def dxT(tensor: torch.Tensor) -> torch.Tensor:\n",
    "\n",
    "    if tensor.requires_grad:\n",
    "        gradT_x = lasp.torch.autograd_ops.TransposedGradX2DCirc.apply(\n",
    "            tensor\n",
    "        )\n",
    "    else:\n",
    "        gradT_x = lasp.torch.differential.dxT2D_circ(\n",
    "            tensor,\n",
    "            detach = True\n",
    "        )\n",
    "\n",
    "    return gradT_x\n",
    "\n",
    "def dy(tensor: torch.Tensor) -> torch.Tensor:\n",
    "\n",
    "    if tensor.requires_grad:\n",
    "        grad_y = lasp.torch.autograd_ops.GradY2DCirc.apply(\n",
    "            tensor\n",
    "        )\n",
    "    else:\n",
    "        grad_y = lasp.torch.differential.dy2D_circ(\n",
    "            tensor,\n",
    "            detach = True\n",
    "        )\n",
    "        \n",
    "    return grad_y\n",
    "\n",
    "def dyT(tensor: torch.Tensor) -> torch.Tensor:\n",
    "\n",
    "    if tensor.requires_grad:\n",
    "        gradT_y = lasp.torch.autograd_ops.TransposedGradY2DCirc.apply(\n",
    "            tensor\n",
    "        )\n",
    "    else:\n",
    "        gradT_y = lasp.torch.differential.dyT2D_circ(\n",
    "            tensor,\n",
    "            detach = True\n",
    "        )\n",
    "        \n",
    "    return gradT_y\n",
    "\n",
    "def multidimensional_soft(d: torch.Tensor, epsilon: float, gamma_zero: float=1e-12):\n",
    "    \"\"\" Thresholding soft for multidimensional array\n",
    "    Use generalization of sign function\n",
    "    \n",
    "    Params:\n",
    "        - d : multidimensional array\n",
    "        - epsilon : threshold\n",
    "        - gamma_zero : for zero value (prevent \"Error detected in DivBackward0\")\n",
    "\n",
    "    Return:\n",
    "        Array thresholded with dimesion equal to d\n",
    "    \"\"\"\n",
    "    #print('d :', d.size())\n",
    "    # l22 = \n",
    "    \n",
    "    # s[s==0] = \n",
    "    #print('s :', s.size())\n",
    "    s = torch.sqrt(torch.sum(d**2, axis=0)+gamma_zero)\n",
    "\n",
    "    ss = torch.where(s > epsilon, (s-epsilon)/s, 0)\n",
    "    output = torch.concat([(ss*d[i]).unsqueeze(0) for i in range(0, d.size()[0])], 0)\n",
    "    #print('output :', output.size())\n",
    "    #print(output.size())\n",
    "    return output\n",
    "\n",
    "\n",
    "class CustomConv2D(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, \n",
    "        in_channels: int, \n",
    "        out_channels: int, \n",
    "        kernel_size: tuple[int, int],\n",
    "        padding: str ='same', \n",
    "        bias: bool = False\n",
    "    ) -> None:\n",
    "        super(CustomConv2D, self).__init__()\n",
    "        self.conv = torch.nn.Conv2d(\n",
    "            in_channels,\n",
    "            out_channels,\n",
    "            kernel_size,\n",
    "            padding = padding,\n",
    "            bias = bias\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.conv(x)\n",
    " \n",
    "class Cell(torch.nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "        alpha: torch.Tensor,\n",
    "        beta1: torch.Tensor,\n",
    "        sigma: torch.Tensor,\n",
    "        kernel_size: tuple[int, int],\n",
    "        nb_intermediate: int\n",
    "    ):\n",
    "        \n",
    "        super(Cell, self).__init__()\n",
    "\n",
    "        self.alpha = alpha\n",
    "        self.beta1 = beta1\n",
    "        self.sigma = sigma\n",
    "\n",
    "        self.HT = CustomConv2D(1, nb_intermediate, kernel_size)\n",
    "        self.inv = CustomConv2D(nb_intermediate, 1, kernel_size)\n",
    "\n",
    "    def forward(self, \n",
    "        STg: torch.Tensor, # (1, N, M)\n",
    "        d_x: torch.Tensor, # (1, N, M)\n",
    "        d_y: torch.Tensor, # (1, N, M)\n",
    "        b_x: torch.Tensor, # (1, N, M)\n",
    "        b_y: torch.Tensor  # (1, N, M)\n",
    "    ) -> torch.Tensor:\n",
    "        \n",
    "        # print(d_x.size())\n",
    "\n",
    "        gradT_x = dxT(d_x - b_x) # (1, N, M)\n",
    "        gradT_y = dyT(d_y - b_y) # (1, N, M)\n",
    "        sigma_expr = self.sigma * ( gradT_x + gradT_y ) # (1, N, M)\n",
    "        alpha_expr = self.alpha * self.HT(STg) # (nb_intermediate, N, M)\n",
    "\n",
    "        f = self.inv(sigma_expr + alpha_expr) # (1, N, M)\n",
    "\n",
    "        dx_f = dx(f) # (1, N, M)\n",
    "        dy_f = dy(f) # (1, N, M)\n",
    "     \n",
    "        d_x, d_y = multidimensional_soft(\n",
    "            torch.concat([ (dx_f + b_x), (dy_f + b_y) ], dim = 0),\n",
    "            self.beta1 / self.sigma\n",
    "        ) # (1, N, M), (1, N, M)\n",
    "      \n",
    "        b_x += (dx_f - d_x) # (1, N, M)\n",
    "        b_y += (dy_f - d_y) # (1, N, M)\n",
    "\n",
    "\n",
    "        return [ f, d_x, d_y, b_x, b_y ]\n",
    "\n",
    "\n",
    "\n",
    "class Unfolding(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, \n",
    "        alpha: tuple[torch.float, bool],\n",
    "        beta1: tuple[torch.float, bool],\n",
    "        sigma: tuple[torch.float, bool],\n",
    "        nb_iterations: int,\n",
    "        kernel_size: tuple[int, int],\n",
    "        nb_intermediate: int\n",
    "    ) -> None:\n",
    "        \n",
    "        super(Unfolding, self).__init__()\n",
    "\n",
    "        self.nb_iterations = nb_iterations\n",
    "\n",
    "        self.alphas = torch.nn.Parameter(\n",
    "            data = torch.fill(\n",
    "                input = torch.zeros(\n",
    "                    size=(self.nb_iterations,), \n",
    "                    dtype=torch.float\n",
    "                ), \n",
    "                value=alpha[0]\n",
    "            ),\n",
    "            requires_grad=alpha[1]\n",
    "        )\n",
    "\n",
    "        self.beta1s = torch.nn.Parameter(\n",
    "            data = torch.fill(\n",
    "                input = torch.zeros(\n",
    "                    size=(self.nb_iterations,), \n",
    "                    dtype=torch.float\n",
    "                ), \n",
    "                value=beta1[0]\n",
    "            ),\n",
    "            requires_grad=beta1[1]\n",
    "        )\n",
    "\n",
    "        self.sigmas = torch.nn.Parameter(\n",
    "            data = torch.fill(\n",
    "                input = torch.zeros(\n",
    "                    size=(self.nb_iterations,), \n",
    "                    dtype=torch.float\n",
    "                ), \n",
    "                value=sigma[0]\n",
    "            ),\n",
    "            requires_grad=sigma[1]\n",
    "        )\n",
    "\n",
    "        self.cells = torch.nn.ModuleList(\n",
    "            [\n",
    "                Cell(\n",
    "                    alpha = self.alphas[i],\n",
    "                    beta1 = self.beta1s[i],\n",
    "                    sigma = self.sigmas[i],\n",
    "                    kernel_size = kernel_size,\n",
    "                    nb_intermediate = nb_intermediate\n",
    "                )\n",
    "                for i in range(0, self.nb_iterations)\n",
    "            ]\n",
    "        )\n",
    "            \n",
    "\n",
    "    def forward(self, low_resolution: torch.Tensor, decim_row: int, decim_col: int) -> torch.Tensor:\n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "            Params:\n",
    "                - low_resolution : image low-resolution; Size([1, N, M])\n",
    "                - decim_row : decimation on line\n",
    "                - decim_col : decimation on col\n",
    "\n",
    "            Return:\n",
    "                Image high-resolution of size.\n",
    "                If size of low_resolution is (N, M), Image high-resolution\n",
    "                will be (N*decim_row, M*decim_col)\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "        # Initialize static attribute / shared attribute\n",
    "\n",
    "        g = low_resolution\n",
    "        S = lasp.torch.utils.Decimation2D(decim_row, decim_col)\n",
    "       \n",
    "\n",
    "        STg = S.T(g)\n",
    "\n",
    "        d_x = torch.zeros_like(STg)\n",
    "        d_y = torch.zeros_like(STg)\n",
    "        b_x = torch.zeros_like(STg)\n",
    "        b_y = torch.zeros_like(STg)\n",
    "\n",
    "        for cell in self.cells:\n",
    "            # STg, d_x, d_y, b_x, b_y = iter_layer(STg, d_x, d_y, b_x, b_y)\n",
    "            f, d_x, d_y, b_x, b_y = cell(STg, d_x, d_y, b_x, b_y)\n",
    "\n",
    "        # f_approx = Iteration.f\n",
    "        f_approx = f\n",
    "        # Normalize f\n",
    "        mini = torch.min(f_approx)\n",
    "        maxi = torch.max(f_approx)\n",
    "        normalized = (f_approx - mini) / (maxi - mini)\n",
    "\n",
    "        return normalized\n",
    "    \n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand(size=(1, 256, 256))\n",
    "decim_row, decim_col = 2, 2\n",
    "y = torch.rand(size=(1, 512, 512))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Unfolding(\n",
    "    alpha = (1e-3, True),\n",
    "    beta1 = (1.0, True),\n",
    "    sigma = (0.2, True),\n",
    "    nb_iterations = 5,\n",
    "    # Convolution parameters\n",
    "    kernel_size = (3, 3),\n",
    "    nb_intermediate = 1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model(x, decim_row, decim_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lasp.torch.loss\n",
    "\n",
    "\n",
    "\n",
    "loss = lasp.torch.loss.mumford_shah_loss(\n",
    "    alpha = model.alphas.mean(),\n",
    "    beta0 = 0.1,\n",
    "    beta1 = model.beta1s.mean(),\n",
    "    y = y,\n",
    "    y_pred = y_pred,\n",
    "    isotropic_mode = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "104559.40625"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.4622, 0.4601, 0.4410,  ..., 0.4849, 0.4435, 0.3850],\n",
       "         [0.5635, 0.6012, 0.6651,  ..., 0.5238, 0.7249, 0.6515],\n",
       "         [0.4170, 0.3386, 0.4298,  ..., 0.1405, 0.4393, 0.3910],\n",
       "         ...,\n",
       "         [0.5781, 0.6274, 0.7559,  ..., 0.5978, 0.8626, 0.6561],\n",
       "         [0.4383, 0.1894, 0.1814,  ..., 0.0818, 0.2378, 0.2429],\n",
       "         [0.6278, 0.5241, 0.8183,  ..., 0.5274, 0.7832, 0.5984]]],\n",
       "       grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.9622, 0.1349, 0.4584,  ..., 0.2740, 0.1316, 0.0774],\n",
       "         [0.1727, 0.4896, 0.9745,  ..., 0.5633, 0.2942, 0.4012],\n",
       "         [0.3686, 0.1163, 0.9242,  ..., 0.7628, 0.4799, 0.3672],\n",
       "         ...,\n",
       "         [0.1912, 0.0528, 0.1072,  ..., 0.6099, 0.2618, 0.3973],\n",
       "         [0.6623, 0.7162, 0.5505,  ..., 0.7641, 0.7130, 0.0056],\n",
       "         [0.3641, 0.9697, 0.9035,  ..., 0.1898, 0.4485, 0.2384]]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.1271, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "((y_pred - y)**2).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1270817369222641"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mse = torch.nn.MSELoss()\n",
    "mse(y_pred, y).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
